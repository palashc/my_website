[{"authors":null,"categories":null,"content":"This feature can be used for publishing content such as:\n Project or software documentation Online courses Tutorials  The parent folder may be renamed, for example, to docs for project documentation or course for creating an online course.\nTo disable this feature, either delete the parent folder, or set draft = true in the front matter of all its pages.\nAfter renaming or deleting the parent folder, you may wish to update any [[menu.main]] menu links to it in the config.toml.\n","date":1536431400,"expirydate":-62135596800,"kind":"section","lang":"en","lastmod":1536431400,"objectID":"c3224f3a64174f08aaf31e1f1d16ffd3","permalink":"/tutorial/","publishdate":"2018-09-09T00:00:00+05:30","relpermalink":"/tutorial/","section":"tutorial","summary":"This feature can be used for publishing content such as:\n Project or software documentation Online courses Tutorials  The parent folder may be renamed, for example, to docs for project documentation or course for creating an online course.\nTo disable this feature, either delete the parent folder, or set draft = true in the front matter of all its pages.\nAfter renaming or deleting the parent folder, you may wish to update any [[menu.","tags":null,"title":"Overview","type":"docs"},{"authors":null,"categories":null,"content":"In recent years, the proliferation of internet technology has created a surge in machine generated events. These events generally have three parts – timestamp, dimensions and metrics. For example – advertising impression data with dimensions like publisher, gender, country etc and metrics like clicks, price etc. Individually these events contain minimal useful information and are of low value. Earlier, companies were willing to discard this data due to the time and resources required to extract any meaning out of it. But now, big data technology such as Hadoop has contributed much to helping companies convert their low value event streams to high value aggregates for a variety of applications.However, Hadoop opens our eyes to new problems. Hadoop is a great batch-processing and data warehousing system but it does not make any performance guarantees and its performance degrades under concurrent workload. If a product’s use case needs guarantees around query performance and data availability in a highly concurrent environment, Hadoop is certainly not going to meet the needs. Using an RDBMS like MySQL or Postgres is not an option since data might be stored in a star schema which will lead to operational overhead and table-scan operations (global counts) will be very slow. Another option is to pre-compute aggregates for all possible combinations of dimensions and store them in a key-value NoSQL store. However, the time required to pre-compute aggregates will increase exponentially with the number of dimensions. While working in the Infrastructure Team at Adobe Media Optimizer, I had to evaluate other big data systems for a similar use-case.Druid is a distributed column-oriented real-time analytical data store which has a parallelized architecture and is optimized for OLAP workflows.Presto is a distributed SQL query engine designed to query large data sets distributed over one or more heterogeneous data sources.\n","date":1541961000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1541961000,"objectID":"dbff3252a5509376ca20498383df295a","permalink":"/project/evaluating-druid-and-presto/","publishdate":"2018-11-12T00:00:00+05:30","relpermalink":"/project/evaluating-druid-and-presto/","section":"project","summary":"Studying the design of Druid and Presto and benchmarking them on certain use-cases.","tags":["Systems","Databases"],"title":"Evaluating Druid and Presto","type":"project"},{"authors":null,"categories":null,"content":" In this tutorial, I\u0026rsquo;ll share my top 10 tips for getting started with Academic:\nTip 1 \u0026hellip;\nTip 2 \u0026hellip;\n","date":1536431400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1536431400,"objectID":"6a451186c775f5f0adb3a0416d0cb711","permalink":"/tutorial/example/","publishdate":"2018-09-09T00:00:00+05:30","relpermalink":"/tutorial/example/","section":"tutorial","summary":"In this tutorial, I\u0026rsquo;ll share my top 10 tips for getting started with Academic:\nTip 1 \u0026hellip;\nTip 2 \u0026hellip;","tags":null,"title":"Example Page","type":"docs"},{"authors":null,"categories":null,"content":"Data dependencies can severely hinder Instruction Level Parallelism (ILP). Data dependencies decrease ILP when long latency instructions flow through the pipeline, and there are not enough independent instructions available to keep the processor busy. Data dependent instructions will stall behind these long latency instructions, potentially creating one or several critical paths through a portion of the program. Load Value Prediction (LVP) is an approach that breaks data dependency on load instructions by predicting the value which is fetched by the instruction. LVP exploits value locality which can be defined as the likelihood of a previously-seen value recurring repeatedly within a storage location. In this project, we explore this notion of value locality, exploit it in order to perform load value prediction and analyze the performance on SPEC2006 Benchmarks.\n","date":1494527400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1494527400,"objectID":"7e3f76f457258ccde03c527323493a25","permalink":"/project/load-value-prediction/","publishdate":"2017-05-12T00:00:00+05:30","relpermalink":"/project/load-value-prediction/","section":"project","summary":"Predicting values loaded by machine intructions to aid Instruction Level Parallelism.","tags":["Systems","Computer Architecture"],"title":"Load Value Prediction","type":"project"},{"authors":null,"categories":null,"content":"Recommendation systems are a vital component of the modern web. They help readers effectively navigate otherwise unwieldy archives of information and help websites direct users to items - movies, articles, songs, products that they will like. Collaborative filtering is one of the techniques used for building recommendation systems which involves inferring user preferences and item attributes from data.  In this project, I studied various models for Bayesian Recommender Systems including Poisson Matrix Factorization and its extensions like Hierarchical Poisson Matrix Factorization and Bayesian Non-parametric Poisson Matrix Factorization. I analyzed the effect of latent dimensions on the models and learnt the use of auxiliary variables in variational inference to make the models locally conjugate and facilitate inference. I also evaluated their performance on MovieLens 1M dataset.\n","date":1491762600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1491762600,"objectID":"8122b93483c65e5488c1d4845fedaa5c","permalink":"/project/poisson/","publishdate":"2017-04-10T00:00:00+05:30","relpermalink":"/project/poisson/","section":"project","summary":"Probabilistic Models for Bayesion Recommender Systems.","tags":["Bayesian ML","Machine Learning"],"title":"Poisson Matrix Factorization","type":"project"},{"authors":["Palash Chauhan","Aditya Gupta","Naman Jain","Sanjeev Biswas"],"categories":null,"content":"","date":1488306600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1488306600,"objectID":"001f1e97233f146a6e82250cca4d6308","permalink":"/publication/topic-models/","publishdate":"2017-03-01T00:00:00+05:30","relpermalink":"/publication/topic-models/","section":"publication","summary":"Computing application range significantly in their perceived complexity. Some applications are immediately intuitive. Most of the users might be able to leverage the functionality of these applications when first exposed to them without much instruction. Other application, however, may require some amount of prior knowledge, learning or experimentation. Such applications have an abundance of functionality buried in menus, accessible through particular dialogs, initiated by particular keyboard shortcuts and so forth. Adobe Creative Cloud products such as Photoshop and llustrator are prime examples of such applications. Cancellation rates are the highest in the first 30-60 days when users are onboarding indicating difficulty in usage. Conventional techniques of informing the users about the functionality of the application involve enabling users to search for the desired functionality and obtain articles/videos which demonstrate the usage of tools. Providing information this way, however, places the responsibility of obtaining the information on the user itself. This may seem tedious to most users and lead to frustration and finally in cancellation of a subscription.To tackle this issue, we propose a system for contextual prediction of user actions in such applications and a method to surface relevant user guidance in the form of text/images/videos relevant to the context in which the user is working. We make an analogy where a user session is seen as a document and the actions taken by the user in a string format are the words. We then model huge volume of user session data using a Topic Model (LDA: Latent Dirichlet Allocation). The extracted topics, which are nothing but distributions over the set of application tools (or subsets of tools considering the top-k tools), can then be interpreted with the help of an expert. The topics are equivalent to ‘sub-workflows’ in an entire user workflow. For example a user working in Illustrator might work with shapes or with text or with images, all of which could be different topics with different set of tools defining them. We also propose a sliding window approach to detect which topic the current user workflow belongs to. Using this approach we build a topic-transition model which can then be used in a live user session to identify what the user is trying to do and surface relevant guidance information. We show that predicting the next few topics the user might move to is more helpful than just predicting the next few tools the user might use. The entire approach can also be extended to achieve a user-base segmentation and tool analysis.","tags":["Machine Learning","Big Data Analysis","Topic Models","Adobe Creative Cloud"],"title":"Application Tool Recommendation","type":"publication"},{"authors":null,"categories":null,"content":"Malwares remain a large problem as attackers use them to disrupt systems which in turn result in costly after effects. Malware detec- tion is mainly carries out using heuristic and signature-based methods which fails to perform due to continuous evolution of different malware families. In the past decade, Deep Learning has shown very strong promises in order to solve any given problem. This project explores ways to detect malwares by extracting features from binaries and using these to train a deep neural network. Experiments were also carried out using AutoEncoders, LSTM and CNN. We also experimented with raw byte sequence as a feature and the results shows that the neural network is able to learn quite good and extract meaningful information with this raw information.\n","date":1487010600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1487010600,"objectID":"b0fab405f8c9890a23077002c9f442f0","permalink":"/project/malware-detection-using-machine-learning/","publishdate":"2017-02-14T00:00:00+05:30","relpermalink":"/project/malware-detection-using-machine-learning/","section":"project","summary":"Predicting whether an execuatable is malware or benign.","tags":["Machine Learning","Systems"],"title":"Malware Detection using Machine Learning","type":"project"},{"authors":[],"categories":null,"content":"Click on the **Slides** button above to view the built-in slides feature.  Slides can be added in a few ways:\n Create slides using Academic\u0026rsquo;s Slides feature and link using url_slides parameter in the front matter of the talk file Upload an existing slide deck to static/ and link using url_slides parameter in the front matter of the talk file Embed your slides (e.g. Google Slides) or presentation video on this page using shortcodes.  Further talk details can easily be added to this page using Markdown and $\\rm \\LaTeX$ math code.\n","date":1483209000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1483209000,"objectID":"96344c08df50a1b693cc40432115cbe3","permalink":"/talk/example/","publishdate":"2017-01-01T00:00:00+05:30","relpermalink":"/talk/example/","section":"talk","summary":"Lorem ipsum dolor sit amet, consectetur adipiscing elit. Duis posuere tellus ac convallis placerat. Proin tincidunt magna sed ex sollicitudin condimentum. Sed ac faucibus dolor, scelerisque sollicitudin nisi. Cras purus urna, suscipit quis sapien eu, pulvinar tempor diam.","tags":[],"title":"Example Talk","type":"talk"},{"authors":null,"categories":null,"content":"Dense captioning is a task which requires a computer vision system to both localize and describe salient regions in images in natural language. The dense captioning task generalizes object detection when the descriptions consist of a single word, and Image Captioning when one predicted region covers the full image. \u0026ldquo;DenseCap: Fully Convolutional Localization Networks for Dense Captioning\u0026rdquo; by Karpathy et. al. proposed a Fully Convolutional Localization Network (FCLN) architecture that processes an image with a single, efficient forward pass, requires no external regions proposals, and can be trained end-to-end with a single round of optimization. The architecture is composed of a Convolutional Network, a novel dense localization layer, and Recurrent Neural Network language model that generates the label sequences. In this project, we tried two things. First was to reproduce the results obtained by the authors to get familiar with the codebase. Second, we replaced the test time Non-Maximal Supression with a Tyrolean Network as described in \u0026ldquo;A convnet for non-maximum suppression\u0026rdquo; by Hosang et. al. We were able to obtain a slight increase in the Mean Average Precision of DenseCap compared to our run of the original code.\n","date":1481394600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1481394600,"objectID":"1f07bf33e356be4db7b1e8faac43f07d","permalink":"/project/dense-image-captioning/","publishdate":"2016-12-11T00:00:00+05:30","relpermalink":"/project/dense-image-captioning/","section":"project","summary":"Teaching a computer vision system to localize and describe salient regions in images in natural language","tags":["Deep Learning","Machine Learning"],"title":"Dense Image Captioning","type":"project"},{"authors":null,"categories":null,"content":"As the problem of information overload grows, and as the amount of data increases, the interest in automatic summarization is also increasing. There has been a lot of reserach work in the area of text summarization. Summarization can be achieved either by extracting elements from the input (Extractive) or by understanding the content of the input and using language generation techniques (Abstractive). Both these methods do not perform well on long documents like research papers. Through this project, we proposed an approach for automatic summarization which is a combination of both these methods. Salient sentences are first extracted from the long document which are then fed to a sequence-to-sequence RNN. We experimented with a number of ways to extract salient elements like LDA, LSA and TextRank and fed the best extraction to an RNN to generate an enhanced summary. We evaluated the generated summaries using the ROUGE metric on a dataset containing research papers from NIPS 2015.\n","date":1481308200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1481308200,"objectID":"665951883af875980f3647943b5b343e","permalink":"/project/automatic-abstract-generation-for-research-papers/","publishdate":"2016-12-10T00:00:00+05:30","relpermalink":"/project/automatic-abstract-generation-for-research-papers/","section":"project","summary":"Text Summarization for Long Documents like research papers.","tags":["Deep Learning","Machine Learning"],"title":"Automatic Abstract Generation for Research Papers","type":"project"},{"authors":[],"categories":null,"content":" Academic makes it easy to create a beautiful website for free using Markdown. Customize anything on your site with widgets, themes, and language packs.\nFollow our easy step by step guide to learn how to build your own free website with Academic. Check out the personal demo or the business demo of what you\u0026rsquo;ll get in less than 10 minutes.\n View the documentation Ask a question Request a feature or report a bug Updating? View the Update Guide and Release Notes Support development of Academic:  Donate a coffee Become a backer on Patreon Decorate your laptop or journal with an Academic sticker Wear the T-shirt   \nKey features:\n Easily manage various content including homepage, blog posts, publications, talks, and projects Extensible via color themes and widgets/plugins Write in Markdown for easy formatting and code highlighting, with LaTeX for mathematical expressions Social/academic network linking, Google Analytics, and Disqus comments Responsive and mobile friendly Simple and refreshing one page design Multilingual and easy to customize  Color Themes Academic is available in different color themes and font themes.\n         Ecosystem  Academic Admin: An admin tool to import publications from BibTeX or import assets for an offline site Academic Scripts: Scripts to help migrate content to new versions of Academic  Install You can choose from one of the following four methods to install:\n one-click install using your web browser (recommended) install on your computer using Git with the Command Prompt/Terminal app install on your computer by downloading the ZIP files install on your computer with RStudio  Quick install using your web browser  Install Academic with Netlify  Netlify will provide you with a customizable URL to access your new site  On GitHub, go to your newly created academic-kickstart repository and edit config.toml to personalize your site. Shortly after saving the file, your site will automatically update Read the Quick Start Guide to learn how to add Markdown content. For inspiration, refer to the Markdown content which powers the Demo  Install with Git Prerequisites:\n Download and install Git Download and install Hugo   Fork the Academic Kickstart repository and clone your fork with Git:\ngit clone https://github.com/sourcethemes/academic-kickstart.git My_Website  Note that if you forked Academic Kickstart, the above command should be edited to clone your fork, i.e. replace sourcethemes with your GitHub username.\n Initialize the theme:\ncd My_Website git submodule update --init --recursive   Install with ZIP  Download and extract Academic Kickstart Download and extract the Academic theme to the themes/academic/ folder from the above step  Install with RStudio View the guide to installing Academic with RStudio\nQuick start  If you installed on your computer, view your new website by running the following command:\nhugo server  Now visit localhost:1313 and your new Academic powered website will appear. Otherwise, if using Netlify, they will provide you with your URL.\n Read the Quick Start Guide to learn how to add Markdown content, customize your site, and deploy it. For inspiration, refer to the Markdown content which powers the Demo\n Build your site by running the hugo command. Then host it for free using Github Pages or Netlify (refer to the first installation method). Alternatively, copy the generated public/ directory (by FTP, Rsync, etc.) to your production web server (such as a university\u0026rsquo;s hosting service).\n  Updating Feel free to star the project on Github to help keep track of updates and check out the release notes prior to updating your site.\nBefore updating the framework, it is recommended to make a backup of your entire website directory (or at least your themes/academic directory) and record your current version number.\nBy default, Academic is installed as a Git submodule which can be updated by running the following command:\ngit submodule update --remote --merge  Check out the update guide for full instructions and alternative methods.\nFeedback \u0026amp; Contributing Please use the issue tracker to let me know about any bugs or feature requests, or alternatively make a pull request.\nFor support, head over to the Hugo discussion forum.\nLicense Copyright 2016-present George Cushen.\nReleased under the MIT license.\n","date":1461090600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1515781800,"objectID":"279b9966ca9cf3121ce924dca452bb1c","permalink":"/post/getting-started/","publishdate":"2016-04-20T00:00:00+05:30","relpermalink":"/post/getting-started/","section":"post","summary":"Create a beautifully simple website or blog in under 10 minutes.","tags":["Academic"],"title":"Academic: the website designer for Hugo","type":"post"},{"authors":null,"categories":null,"content":" Welcome to Slides Academic\nFeatures  Efficiently write slides in Markdown 3-in-1: Create, Present, and Publish your slides Supports speaker notes Mobile friendly slides  Controls  Next: Right Arrow or Space Previous: Left Arrow Start: Home Finish: End Overview: Esc Speaker notes: S Fullscreen: F Zoom: Alt + Click PDF Export: E  Code Highlighting Inline code: variable\nCode block:\nporridge = \u0026quot;blueberry\u0026quot; if porridge == \u0026quot;blueberry\u0026quot;: print(\u0026quot;Eating...\u0026quot;)  Math In-line math: $x + y = z$\nBlock math:\n$$ f\\left( x \\right) = \\;\\frac{{2\\left( {x + 4} \\right)\\left( {x - 4} \\right)}}{{\\left( {x + 4} \\right)\\left( {x + 1} \\right)}} $$\nFragments Make content appear incrementally\n{{% fragment %}} One {{% /fragment %}} {{% fragment %}} **Two** {{% /fragment %}} {{% fragment %}} Three {{% /fragment %}}  Press Space to play!\nOne  Two  Three \nA fragment can accept two optional parameters:\n class: use a custom style (requires definition in custom CSS) weight: sets the order in which a fragment appears  Speaker Notes Add speaker notes to your presentation\n{{% speaker_note %}} - Only the speaker can read these notes - Press `S` key to view {{% /speaker_note %}}  Press the S key to view the speaker notes!\n Only the speaker can read these notes Press S key to view   Themes  black: Black background, white text, blue links (default) white: White background, black text, blue links league: Gray background, white text, blue links beige: Beige background, dark text, brown links sky: Blue background, thin dark text, blue links   night: Black background, thick white text, orange links serif: Cappuccino background, gray text, brown links simple: White background, black text, blue links solarized: Cream-colored background, dark green text, blue links  Custom Slide Customize the slide style and background\n{{\u0026lt; slide background-image=\u0026quot;/img/boards.jpg\u0026quot; \u0026gt;}} {{\u0026lt; slide background-color=\u0026quot;#0000FF\u0026quot; \u0026gt;}} {{\u0026lt; slide class=\u0026quot;my-style\u0026quot; \u0026gt;}}  Custom CSS Example Let\u0026rsquo;s make headers navy colored.\nCreate assets/css/reveal_custom.css with:\n.reveal section h1, .reveal section h2, .reveal section h3 { color: navy; }  Questions? Ask\nDocumentation\n","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"c2915ec5da95791851caafdcba9664af","permalink":"/slides/example-slides/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/slides/example-slides/","section":"slides","summary":"Welcome to Slides Academic\nFeatures  Efficiently write slides in Markdown 3-in-1: Create, Present, and Publish your slides Supports speaker notes Mobile friendly slides  Controls  Next: Right Arrow or Space Previous: Left Arrow Start: Home Finish: End Overview: Esc Speaker notes: S Fullscreen: F Zoom: Alt + Click PDF Export: E  Code Highlighting Inline code: variable\nCode block:\nporridge = \u0026quot;blueberry\u0026quot; if porridge == \u0026quot;blueberry\u0026quot;: print(\u0026quot;Eating...\u0026quot;)  Math In-line math: $x + y = z$","tags":null,"title":"Slides","type":"slides"}]